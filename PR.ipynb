{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\python310\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: langdetect in c:\\python310\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from langdetect) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in c:\\python310\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from langid) (1.26.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\python310\\lib\\site-packages (1.4.6)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from keras-tuner) (2.31.0)\n",
      "Requirement already satisfied: keras in c:\\python310\\lib\\site-packages (from keras-tuner) (2.15.0)\n",
      "Requirement already satisfied: kt-legacy in c:\\python310\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from keras-tuner) (23.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->keras-tuner) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->keras-tuner) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\python310\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\python310\\lib\\site-packages (from seaborn) (1.26.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\python310\\lib\\site-packages (from seaborn) (3.8.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\python310\\lib\\site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.46.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in c:\\python310\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from langid) (1.26.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10337, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nature, in the broadest sense, is the natural...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Nature\" can refer to the phenomena of the phy...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The study of nature is a large, if not the onl...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although humans are part of nature, human acti...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1] The word nature is borrowed from the Old F...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10332</th>\n",
       "      <td>ನಿಮ್ಮ ತಪ್ಪು ಏನು ಬಂದಿದೆಯೆಂದರೆ ಆ ದಿನದಿಂದ ನಿಮಗೆ ಒ...</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10333</th>\n",
       "      <td>ನಾರ್ಸಿಸಾ ತಾನು ಮೊದಲಿಗೆ ಹೆಣಗಾಡುತ್ತಿದ್ದ ಮಾರ್ಗಗಳನ್...</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10334</th>\n",
       "      <td>ಹೇಗೆ ' ನಾರ್ಸಿಸಿಸಮ್ ಈಗ ಮರಿಯನ್ ಅವರಿಗೆ ಸಂಭವಿಸಿದ ಎ...</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335</th>\n",
       "      <td>ಅವಳು ಈಗ ಹೆಚ್ಚು ಚಿನ್ನದ ಬ್ರೆಡ್ ಬಯಸುವುದಿಲ್ಲ ಎಂದು ...</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10336</th>\n",
       "      <td>ಟೆರ್ರಿ ನೀವು ನಿಜವಾಗಿಯೂ ಆ ದೇವದೂತನಂತೆ ಸ್ವಲ್ಪ ಕಾಣು...</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10337 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Language\n",
       "0       Nature, in the broadest sense, is the natural...  English\n",
       "1      \"Nature\" can refer to the phenomena of the phy...  English\n",
       "2      The study of nature is a large, if not the onl...  English\n",
       "3      Although humans are part of nature, human acti...  English\n",
       "4      [1] The word nature is borrowed from the Old F...  English\n",
       "...                                                  ...      ...\n",
       "10332  ನಿಮ್ಮ ತಪ್ಪು ಏನು ಬಂದಿದೆಯೆಂದರೆ ಆ ದಿನದಿಂದ ನಿಮಗೆ ಒ...  Kannada\n",
       "10333  ನಾರ್ಸಿಸಾ ತಾನು ಮೊದಲಿಗೆ ಹೆಣಗಾಡುತ್ತಿದ್ದ ಮಾರ್ಗಗಳನ್...  Kannada\n",
       "10334  ಹೇಗೆ ' ನಾರ್ಸಿಸಿಸಮ್ ಈಗ ಮರಿಯನ್ ಅವರಿಗೆ ಸಂಭವಿಸಿದ ಎ...  Kannada\n",
       "10335  ಅವಳು ಈಗ ಹೆಚ್ಚು ಚಿನ್ನದ ಬ್ರೆಡ್ ಬಯಸುವುದಿಲ್ಲ ಎಂದು ...  Kannada\n",
       "10336  ಟೆರ್ರಿ ನೀವು ನಿಜವಾಗಿಯೂ ಆ ದೇವದೂತನಂತೆ ಸ್ವಲ್ಪ ಕಾಣು...  Kannada\n",
       "\n",
       "[10337 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"LanguageDetection.csv\")\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10337 entries, 0 to 10336\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Text      10337 non-null  object\n",
      " 1   Language  10337 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 161.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language\n",
       "English       1385\n",
       "French        1014\n",
       "Spanish        819\n",
       "Portugeese     739\n",
       "Italian        698\n",
       "Russian        692\n",
       "Sweedish       676\n",
       "Malayalam      594\n",
       "Dutch          546\n",
       "Arabic         536\n",
       "Turkish        474\n",
       "German         470\n",
       "Tamil          469\n",
       "Danish         428\n",
       "Kannada        369\n",
       "Greek          365\n",
       "Hindi           63\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6442</th>\n",
       "      <td>извините, что беспокою вас.</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>Позвольте мне закончить.</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Text Language\n",
       "6442  извините, что беспокою вас.  Russian\n",
       "6561     Позвольте мне закончить.  Russian"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Language == 'Russian'].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>കാരണം നമ്മുടെ പത്രങ്ങൾ ഒക്കെ ഇപ്പോഴും യൂണികോഡ്...</td>\n",
       "      <td>Malayalam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>വർഗ്ഗ ശരാശരി നഷ്ടം ( Mean Squared Error), ആക്യ...</td>\n",
       "      <td>Malayalam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text   Language\n",
       "1450  കാരണം നമ്മുടെ പത്രങ്ങൾ ഒക്കെ ഇപ്പോഴും യൂണികോഡ്...  Malayalam\n",
       "1607  വർഗ്ഗ ശരാശരി നഷ്ടം ( Mean Squared Error), ആക്യ...  Malayalam"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Language == 'Malayalam'].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>لا ، شكرًا على أي حال ، وعبارة أخرى أستخدمها ف...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9315</th>\n",
       "      <td>لقد أخافت عبارة أخرى يستخدمها المتحدثون الأصلي...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Language\n",
       "9268  لا ، شكرًا على أي حال ، وعبارة أخرى أستخدمها ف...   Arabic\n",
       "9315  لقد أخافت عبارة أخرى يستخدمها المتحدثون الأصلي...   Arabic"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Language == 'Arabic'].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>மிகவும் பயமாக இருந்தது என் அன்பே கவலைப்படாதே இ...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>[75] 2007-இல், அச்சுவழிப் பதிப்பொன்றை உருவாக்க...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Language\n",
       "2489  மிகவும் பயமாக இருந்தது என் அன்பே கவலைப்படாதே இ...    Tamil\n",
       "2133  [75] 2007-இல், அச்சுவழிப் பதிப்பொன்றை உருவாக்க...    Tamil"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Language == 'Tamil'].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>ಉದಾಹರಣೆಗೆ ಅವನ ಟೇಬಲ್ ನಡವಳಿಕೆ ದೌರ್ಜನ್ಯ ಅಥವಾ ಅವಳ ...</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10175</th>\n",
       "      <td>ಅದು ನಿಮ್ಮನ್ನು ನಂತರ ನೋಡುವ ಸಂತೋಷದ ಸಭೆ.</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Language\n",
       "9991   ಉದಾಹರಣೆಗೆ ಅವನ ಟೇಬಲ್ ನಡವಳಿಕೆ ದೌರ್ಜನ್ಯ ಅಥವಾ ಅವಳ ...  Kannada\n",
       "10175               ಅದು ನಿಮ್ಮನ್ನು ನಂತರ ನೋಡುವ ಸಂತೋಷದ ಸಭೆ.  Kannada"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Language == 'Kannada'].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>कैसे कुछ आइसक्रीम के बारे में?</td>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>मुझे लगता है कि यह एक अच्छा विचार है। मैं ईमान...</td>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Language\n",
       "2038                     कैसे कुछ आइसक्रीम के बारे में?    Hindi\n",
       "2024  मुझे लगता है कि यह एक अच्छा विचार है। मैं ईमान...    Hindi"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Language == 'Hindi'].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from Language Detection.csv\n",
    "file_path = 'LanguageDetection.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove null values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "output_path = 'cleaned_data.csv'\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10337, 2)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"cleaned_data.csv\")\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python310\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: click in c:\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Read the data from LanguageDataset.csv\n",
    "file_path = 'LanguageDetection.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the column names are as expected ('Text' and 'Language')\n",
    "df.columns = df.columns.str.strip()  # Remove leading/trailing whitespaces\n",
    "\n",
    "# Make sure to replace 'Text' with the actual column name in your dataset\n",
    "text_column_name = 'Text'\n",
    "\n",
    "# Function for text cleaning and preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, and other non-text elements\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply text preprocessing to the specified column\n",
    "df['Text'] = df[text_column_name].apply(preprocess_text).apply(word_tokenize)\n",
    "\n",
    "# Create a new DataFrame with only the 'tokens' and 'Language' columns\n",
    "df_output = df[['Text', 'Language']]\n",
    "# df_output.rename('Text')\n",
    "# Save the preprocessed data to a new CSV file\n",
    "output_path = 'preprocessed.csv'\n",
    "df_output.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python310\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words (BoW) representation:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "TF-IDF representation:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], df['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Bag-of-Words (BoW) representation\n",
    "bow_vectorizer = CountVectorizer()\n",
    "train_bow = bow_vectorizer.fit_transform(train_data)\n",
    "test_bow = bow_vectorizer.transform(test_data)\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Display the feature matrices\n",
    "print(\"Bag-of-Words (BoW) representation:\")\n",
    "print(train_bow.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF representation:\")\n",
    "print(train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSymbolsAndNumbers(text):        \n",
    "        text = re.sub(r'[{}]'.format(string.punctuation), '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'[@]', '', text)\n",
    "\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEnglishLetters(text):        \n",
    "        text = re.sub(r'[a-zA-Z]+', '', text)\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ['nature', 'in', 'the', 'broadest', 'sense', '...\n",
       "1        ['nature', 'can', 'refer', 'to', 'the', 'pheno...\n",
       "2        ['the', 'study', 'of', 'nature', 'is', 'a', 'l...\n",
       "3        ['although', 'humans', 'are', 'part', 'of', 'n...\n",
       "4        ['the', 'word', 'nature', 'is', 'borrowed', 'f...\n",
       "                               ...                        \n",
       "10332                                                   []\n",
       "10333                                                   []\n",
       "10334                                                   []\n",
       "10335                                                 ['']\n",
       "10336                                                   []\n",
       "Length: 10337, dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = df.apply(lambda x: removeEnglishLetters(x.Text) if x.Language in ['Russian','Malyalam','Hindi','Kannada','Tamil','Arabic']  else x.Text, axis = 1)\n",
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        nature in the broadest sense is the natural ph...\n",
       "1        nature can refer to the phenomena of the physi...\n",
       "2        the study of nature is a large if not the only...\n",
       "3        although humans are part of nature human activ...\n",
       "4        the word nature is borrowed from the old frenc...\n",
       "                               ...                        \n",
       "10332                                                     \n",
       "10333                                                     \n",
       "10334                                                     \n",
       "10335                                                     \n",
       "10336                                                     \n",
       "Length: 10337, dtype: object"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = X0.apply(removeSymbolsAndNumbers)\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequency Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv('LanguageDetection.csv')\n",
    "\n",
    "# Extract features and labels, encode labels, and clean text data\n",
    "X = df['Text']\n",
    "y = df['Language']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "df_list = [re.sub(r'[!@#$(),n\"%^*?:;~`0-9]', '', text).lower() for text in X]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_list, y, test_size=0.2, random_state=41)\n",
    "\n",
    "# Feature Extraction: Word Frequency\n",
    "word_vectorizer = CountVectorizer()\n",
    "X_train_word = word_vectorizer.fit_transform(X_train)\n",
    "X_test_word = word_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model and evaluate its accuracy\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_word, y_train)\n",
    "y_pred_word = model.predict(X_test_word)\n",
    "accuracy_word = f1_score(y_test, y_pred_word, average='macro')\n",
    "print(f'Word Frequency Accuracy: {accuracy_word:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character N-grams Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('LanguageDetection.csv')\n",
    "\n",
    "# Extract features and labels, encode labels, and clean text data\n",
    "X = df['Text']\n",
    "y = df['Language']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "df_list = [re.sub(r'[!@#$(),n\"%^*?:;~`0-9]', '', text).lower() for text in X]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_list, y, test_size=0.2, random_state=41)\n",
    "\n",
    "# Feature Extraction: Character N-grams\n",
    "char_ngram_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=5000)  # Adjust max_features as needed\n",
    "X_train_char_ngram = char_ngram_vectorizer.fit_transform(X_train)\n",
    "X_test_char_ngram = char_ngram_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model and evaluate its accuracy\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_char_ngram, y_train)\n",
    "y_pred_char_ngram = model.predict(X_test_char_ngram)\n",
    "accuracy_char_ngram = f1_score(y_test, y_pred_char_ngram, average='macro')\n",
    "print(f'Character N-grams Accuracy: {accuracy_char_ngram:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Features Accuracy: 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('LanguageDetection.csv')\n",
    "\n",
    "# Extract features and labels, encode labels, and clean text data\n",
    "X = df['Text']\n",
    "y = df['Language']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "df_list = [re.sub(r'[!@#$(),n\"%^*?:;~`0-9]', '', text).lower() for text in X]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_list, y, test_size=0.2, random_state=41)\n",
    "\n",
    "# Feature Extraction: Statistical Features (e.g., mean, std, min, max)\n",
    "word_length = [len(word.split()) for word in X_train]\n",
    "word_length_test = [len(word.split()) for word in X_test]\n",
    "\n",
    "X_train_statistical = pd.DataFrame({\n",
    "    'word_length_mean': [pd.Series(word_length).mean()] * len(X_train),\n",
    "    'word_length_std': [pd.Series(word_length).std()] * len(X_train),\n",
    "    'word_length_min': [pd.Series(word_length).min()] * len(X_train),\n",
    "    'word_length_max': [pd.Series(word_length).max()] * len(X_train),\n",
    "})\n",
    "\n",
    "X_test_statistical = pd.DataFrame({\n",
    "    'word_length_mean': [pd.Series(word_length_test).mean()] * len(X_test),\n",
    "    'word_length_std': [pd.Series(word_length_test).std()] * len(X_test),\n",
    "    'word_length_min': [pd.Series(word_length_test).min()] * len(X_test),\n",
    "    'word_length_max': [pd.Series(word_length_test).max()] * len(X_test),\n",
    "})\n",
    "\n",
    "# Train a logistic regression model and evaluate its accuracy\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_statistical, y_train)\n",
    "y_pred_statistical = model.predict(X_test_statistical)\n",
    "accuracy_statistical = f1_score(y_test, y_pred_statistical, average='macro')\n",
    "print(f'Statistical Features Accuracy: {accuracy_statistical:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.02      0.04       106\n",
      "      Danish       0.94      0.88      0.91        73\n",
      "       Dutch       1.00      0.93      0.96       111\n",
      "     English       0.97      0.97      0.97       291\n",
      "      French       0.99      0.93      0.96       219\n",
      "      German       0.99      0.92      0.96        93\n",
      "       Greek       0.25      0.01      0.03        68\n",
      "       Hindi       0.00      0.00      0.00        10\n",
      "     Italian       1.00      0.92      0.96       145\n",
      "     Kannada       0.00      0.00      0.00        66\n",
      "   Malayalam       0.00      0.00      0.00       121\n",
      "  Portugeese       0.99      0.92      0.96       144\n",
      "     Russian       0.20      0.99      0.33       136\n",
      "     Spanish       0.97      0.93      0.95       160\n",
      "    Sweedish       0.98      0.88      0.93       133\n",
      "       Tamil       0.00      0.00      0.00        87\n",
      "     Turkish       1.00      0.80      0.89       105\n",
      "\n",
      "    accuracy                           0.72      2068\n",
      "   macro avg       0.66      0.60      0.58      2068\n",
      "weighted avg       0.77      0.72      0.70      2068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Read the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], df['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Initialize and train the Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='linear')  # You can experiment with different kernels (linear, rbf, etc.)\n",
    "svm_model.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = svm_model.predict(test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "classification_report_result = classification_report(test_labels, predictions)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Accuracy after hyperparameter tuning: 0.65\n",
      "\n",
      "Classification Report after hyperparameter tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00         7\n",
      "      Danish       1.00      0.57      0.73         7\n",
      "       Dutch       1.00      0.83      0.91        12\n",
      "     English       0.97      1.00      0.98        29\n",
      "      French       1.00      0.89      0.94        27\n",
      "      German       1.00      0.80      0.89         5\n",
      "       Greek       0.00      0.00      0.00        10\n",
      "       Hindi       0.00      0.00      0.00         1\n",
      "     Italian       1.00      0.70      0.82        10\n",
      "     Kannada       0.00      0.00      0.00         8\n",
      "   Malayalam       0.00      0.00      0.00        11\n",
      "  Portugeese       0.83      0.77      0.80        13\n",
      "     Russian       0.14      1.00      0.24        11\n",
      "     Spanish       1.00      0.81      0.90        16\n",
      "    Sweedish       0.92      0.80      0.86        15\n",
      "       Tamil       0.00      0.00      0.00         9\n",
      "     Turkish       1.00      0.69      0.81        16\n",
      "\n",
      "    accuracy                           0.65       207\n",
      "   macro avg       0.58      0.52      0.52       207\n",
      "weighted avg       0.71      0.65      0.66       207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Read a sample of the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Reduce the size of the dataset for quicker experimentation\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df_sample['Text'], df_sample['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV with reduced parameters\n",
    "param_grid = {'C': [0.1, 1], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "grid_search.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Initialize and train the Support Vector Machine (SVM) model with best hyperparameters\n",
    "best_svm_model = SVC(**best_params)\n",
    "best_svm_model.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Predictions on the test set using the tuned model\n",
    "tuned_predictions = best_svm_model.predict(test_tfidf)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "tuned_accuracy = accuracy_score(test_labels, tuned_predictions)\n",
    "tuned_classification_report_result = classification_report(test_labels, tuned_predictions)\n",
    "\n",
    "# Display evaluation metrics after hyperparameter tuning\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"Accuracy after hyperparameter tuning: {tuned_accuracy:.2f}\")\n",
    "print(\"\\nClassification Report after hyperparameter tuning:\")\n",
    "print(tuned_classification_report_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before hyperparameter tuning: 0.72\n",
      "\n",
      "Classification Report before hyperparameter tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.02      0.04       106\n",
      "      Danish       0.94      0.88      0.91        73\n",
      "       Dutch       1.00      0.93      0.96       111\n",
      "     English       0.97      0.97      0.97       291\n",
      "      French       0.99      0.93      0.96       219\n",
      "      German       0.99      0.92      0.96        93\n",
      "       Greek       0.25      0.01      0.03        68\n",
      "       Hindi       0.00      0.00      0.00        10\n",
      "     Italian       1.00      0.92      0.96       145\n",
      "     Kannada       0.00      0.00      0.00        66\n",
      "   Malayalam       0.00      0.00      0.00       121\n",
      "  Portugeese       0.99      0.92      0.96       144\n",
      "     Russian       0.20      0.99      0.33       136\n",
      "     Spanish       0.97      0.93      0.95       160\n",
      "    Sweedish       0.98      0.88      0.93       133\n",
      "       Tamil       0.00      0.00      0.00        87\n",
      "     Turkish       1.00      0.80      0.89       105\n",
      "\n",
      "    accuracy                           0.72      2068\n",
      "   macro avg       0.66      0.60      0.58      2068\n",
      "weighted avg       0.77      0.72      0.70      2068\n",
      "\n",
      "\n",
      "Best Hyperparameters: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Accuracy after hyperparameter tuning: 0.65\n",
      "\n",
      "Classification Report after hyperparameter tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00         7\n",
      "      Danish       1.00      0.57      0.73         7\n",
      "       Dutch       1.00      0.83      0.91        12\n",
      "     English       0.97      1.00      0.98        29\n",
      "      French       1.00      0.89      0.94        27\n",
      "      German       1.00      0.80      0.89         5\n",
      "       Greek       0.00      0.00      0.00        10\n",
      "       Hindi       0.00      0.00      0.00         1\n",
      "     Italian       1.00      0.70      0.82        10\n",
      "     Kannada       0.00      0.00      0.00         8\n",
      "   Malayalam       0.00      0.00      0.00        11\n",
      "  Portugeese       0.83      0.77      0.80        13\n",
      "     Russian       0.14      1.00      0.24        11\n",
      "     Spanish       1.00      0.81      0.90        16\n",
      "    Sweedish       0.92      0.80      0.86        15\n",
      "       Tamil       0.00      0.00      0.00         9\n",
      "     Turkish       1.00      0.69      0.81        16\n",
      "\n",
      "    accuracy                           0.65       207\n",
      "   macro avg       0.58      0.52      0.52       207\n",
      "weighted avg       0.71      0.65      0.66       207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display evaluation metrics before hyperparameter tuning\n",
    "print(f\"Accuracy before hyperparameter tuning: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report before hyperparameter tuning:\")\n",
    "print(classification_report_result)\n",
    "\n",
    "# Display evaluation metrics after hyperparameter tuning\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"Accuracy after hyperparameter tuning: {tuned_accuracy:.2f}\")\n",
    "print(\"\\nClassification Report after hyperparameter tuning:\")\n",
    "print(tuned_classification_report_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in c:\\python310\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\python310\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (63.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.25.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python310\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python310\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "233/233 [==============================] - 23s 88ms/step - loss: 2.7683 - accuracy: 0.1180 - val_loss: 2.7382 - val_accuracy: 0.1282\n",
      "Epoch 2/5\n",
      "233/233 [==============================] - 21s 91ms/step - loss: 2.7459 - accuracy: 0.1277 - val_loss: 2.7381 - val_accuracy: 0.1282\n",
      "Epoch 3/5\n",
      "233/233 [==============================] - 24s 102ms/step - loss: 2.7430 - accuracy: 0.1286 - val_loss: 2.7388 - val_accuracy: 0.1282\n",
      "Epoch 4/5\n",
      "233/233 [==============================] - 26s 110ms/step - loss: 2.7370 - accuracy: 0.1330 - val_loss: 2.7351 - val_accuracy: 0.1282\n",
      "Epoch 5/5\n",
      "233/233 [==============================] - 26s 111ms/step - loss: 2.7349 - accuracy: 0.1295 - val_loss: 2.7344 - val_accuracy: 0.1282\n",
      "65/65 [==============================] - 3s 44ms/step - loss: 2.7122 - accuracy: 0.1407\n",
      "Test Accuracy: 0.14\n",
      "65/65 [==============================] - 3s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Read the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], df['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100  # Define the maximum sequence length\n",
    "\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=maxlen, padding='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels)\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Building the neural network model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=maxlen))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Output layer\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(train_sequences_padded, train_labels_encoded, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(test_sequences_padded, test_labels_encoded)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_sequences_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "233/233 [==============================] - 31s 123ms/step - loss: 2.7755 - accuracy: 0.1148 - val_loss: 2.7470 - val_accuracy: 0.1282\n",
      "Epoch 2/5\n",
      "233/233 [==============================] - 28s 120ms/step - loss: 2.7472 - accuracy: 0.1322 - val_loss: 2.7344 - val_accuracy: 0.1282\n",
      "Epoch 3/5\n",
      "233/233 [==============================] - 26s 112ms/step - loss: 2.7439 - accuracy: 0.1299 - val_loss: 2.7331 - val_accuracy: 0.1282\n",
      "Epoch 4/5\n",
      "233/233 [==============================] - 24s 101ms/step - loss: 2.7368 - accuracy: 0.1322 - val_loss: 2.7370 - val_accuracy: 0.1282\n",
      "Epoch 5/5\n",
      "233/233 [==============================] - 21s 89ms/step - loss: 2.7366 - accuracy: 0.1325 - val_loss: 2.7302 - val_accuracy: 0.1282\n",
      "65/65 [==============================] - 1s 17ms/step\n",
      "Accuracy: 0.14\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       106\n",
      "           1       0.00      0.00      0.00        73\n",
      "           2       0.00      0.00      0.00       111\n",
      "           3       0.14      1.00      0.25       291\n",
      "           4       0.00      0.00      0.00       219\n",
      "           5       0.00      0.00      0.00        93\n",
      "           6       0.00      0.00      0.00        68\n",
      "           7       0.00      0.00      0.00        10\n",
      "           8       0.00      0.00      0.00       145\n",
      "           9       0.00      0.00      0.00        66\n",
      "          10       0.00      0.00      0.00       121\n",
      "          11       0.00      0.00      0.00       144\n",
      "          12       0.00      0.00      0.00       136\n",
      "          13       0.00      0.00      0.00       160\n",
      "          14       0.00      0.00      0.00       133\n",
      "          15       0.00      0.00      0.00        87\n",
      "          16       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.14      2068\n",
      "   macro avg       0.01      0.06      0.01      2068\n",
      "weighted avg       0.02      0.14      0.03      2068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Read the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], df['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100  # Define the maximum sequence length\n",
    "\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=maxlen, padding='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels)\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Building the neural network model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=maxlen))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Output layer\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(train_sequences_padded, train_labels_encoded, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = model.predict(test_sequences_padded)\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "predicted_labels_decoded = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels_encoded, predicted_labels)\n",
    "classification_report_result = classification_report(test_labels_encoded, predicted_labels)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\my_project\\tuner0.json"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "24/24 [==============================] - 4s 100ms/step - loss: 2.8096 - accuracy: 0.1102 - val_loss: 2.7471 - val_accuracy: 0.1325\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 2.7652 - accuracy: 0.1384 - val_loss: 2.7490 - val_accuracy: 0.1325\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 2.7335 - accuracy: 0.1519 - val_loss: 2.7152 - val_accuracy: 0.1325\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 2s 78ms/step - loss: 2.7527 - accuracy: 0.1237 - val_loss: 2.7260 - val_accuracy: 0.1325\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 2.7319 - accuracy: 0.1452 - val_loss: 2.7114 - val_accuracy: 0.1325\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 2.7316 - accuracy: 0.1401\n",
      "Test accuracy: 0.1401\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "# Read a sample of the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "df_sample = df.sample(frac=0.1, random_state=42)  # Reduce dataset size\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df_sample['Text'], df_sample['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100  # Define the maximum sequence length\n",
    "\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=maxlen, padding='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels)\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Define a function to create a neural network model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=hp.Int('embedding_size', 50, 200, step=50), input_length=maxlen))\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', 64, 256, step=32)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate the RandomSearch tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # Adjust this based on computational resources\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',  # Directory to save results\n",
    "    project_name='my_project'\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "tuner.search(train_sequences_padded, train_labels_encoded, epochs=5, validation_split=0.1)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the best model using the best hyperparameters found\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "best_model.fit(train_sequences_padded, train_labels_encoded, epochs=5, validation_split=0.1)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(test_sequences_padded, test_labels_encoded)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00         7\n",
      "      Danish       1.00      0.14      0.25         7\n",
      "       Dutch       1.00      0.50      0.67        12\n",
      "     English       0.91      1.00      0.95        29\n",
      "      French       0.81      0.81      0.81        27\n",
      "      German       1.00      0.80      0.89         5\n",
      "       Greek       0.00      0.00      0.00        10\n",
      "       Hindi       0.00      0.00      0.00         1\n",
      "     Italian       0.88      0.70      0.78        10\n",
      "     Kannada       0.00      0.00      0.00         8\n",
      "   Malayalam       0.00      0.00      0.00        11\n",
      "  Portugeese       0.58      0.85      0.69        13\n",
      "     Russian       0.14      1.00      0.25        11\n",
      "     Spanish       1.00      0.44      0.61        16\n",
      "    Sweedish       0.87      0.87      0.87        15\n",
      "       Tamil       0.00      0.00      0.00         9\n",
      "     Turkish       1.00      0.69      0.81        16\n",
      "\n",
      "    accuracy                           0.59       207\n",
      "   macro avg       0.54      0.46      0.45       207\n",
      "weighted avg       0.65      0.59      0.57       207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Read a sample of the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Reduce the size of the dataset for quicker experimentation\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df_sample['Text'], df_sample['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF representation with reduced max_features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)  # Adjust max_features as needed\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Initialize and train the Random Forest classifier with reduced depth and estimators\n",
    "rf_model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)  # Adjust parameters as needed\n",
    "rf_model.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = rf_model.predict(test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "classification_report_result = classification_report(test_labels, predictions)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00       106\n",
      "      Danish       1.00      0.53      0.70        73\n",
      "       Dutch       1.00      0.60      0.75       111\n",
      "     English       0.24      1.00      0.39       291\n",
      "      French       0.98      0.90      0.94       219\n",
      "      German       1.00      0.53      0.69        93\n",
      "       Greek       0.00      0.00      0.00        68\n",
      "       Hindi       0.00      0.00      0.00        10\n",
      "     Italian       1.00      0.75      0.86       145\n",
      "     Kannada       0.00      0.00      0.00        66\n",
      "   Malayalam       0.00      0.00      0.00       121\n",
      "  Portugeese       1.00      0.81      0.90       144\n",
      "     Russian       0.00      0.00      0.00       136\n",
      "     Spanish       0.98      0.79      0.87       160\n",
      "    Sweedish       0.94      0.79      0.86       133\n",
      "       Tamil       0.00      0.00      0.00        87\n",
      "     Turkish       1.00      0.44      0.61       105\n",
      "\n",
      "    accuracy                           0.55      2068\n",
      "   macro avg       0.54      0.42      0.44      2068\n",
      "weighted avg       0.60      0.55      0.53      2068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Read the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], df['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Initialize and train the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = rf_model.predict(test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "classification_report_result = classification_report(test_labels, predictions)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python310\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00       106\n",
      "      Danish       0.98      0.88      0.93        73\n",
      "       Dutch       0.99      0.95      0.97       111\n",
      "     English       0.32      1.00      0.48       291\n",
      "      French       0.96      0.99      0.98       219\n",
      "      German       1.00      0.95      0.97        93\n",
      "       Greek       0.00      0.00      0.00        68\n",
      "       Hindi       0.00      0.00      0.00        10\n",
      "     Italian       1.00      0.97      0.99       145\n",
      "     Kannada       0.00      0.00      0.00        66\n",
      "   Malayalam       0.00      0.00      0.00       121\n",
      "  Portugeese       0.99      0.95      0.97       144\n",
      "     Russian       0.75      0.04      0.08       136\n",
      "     Spanish       0.97      0.97      0.97       160\n",
      "    Sweedish       0.96      0.96      0.96       133\n",
      "       Tamil       0.00      0.00      0.00        87\n",
      "     Turkish       1.00      0.83      0.91       105\n",
      "\n",
      "    accuracy                           0.69      2068\n",
      "   macro avg       0.58      0.56      0.54      2068\n",
      "weighted avg       0.66      0.69      0.62      2068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Read the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], df['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Initialize and train the Naive Bayes (Multinomial) model\n",
    "nb_model = MultinomialNB()  # Using Multinomial Naive Bayes\n",
    "nb_model.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = nb_model.predict(test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "classification_report_result = classification_report(test_labels, predictions)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "Accuracy after hyperparameter tuning: 0.68\n",
      "\n",
      "Classification Report after hyperparameter tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00         7\n",
      "      Danish       1.00      0.86      0.92         7\n",
      "       Dutch       1.00      1.00      1.00        12\n",
      "     English       0.33      1.00      0.50        29\n",
      "      French       0.96      1.00      0.98        27\n",
      "      German       1.00      1.00      1.00         5\n",
      "       Greek       0.00      0.00      0.00        10\n",
      "       Hindi       0.00      0.00      0.00         1\n",
      "     Italian       1.00      0.90      0.95        10\n",
      "     Kannada       0.00      0.00      0.00         8\n",
      "   Malayalam       0.00      0.00      0.00        11\n",
      "  Portugeese       0.80      0.92      0.86        13\n",
      "     Russian       1.00      0.09      0.17        11\n",
      "     Spanish       0.92      0.75      0.83        16\n",
      "    Sweedish       0.94      1.00      0.97        15\n",
      "       Tamil       0.00      0.00      0.00         9\n",
      "     Turkish       0.93      0.81      0.87        16\n",
      "\n",
      "    accuracy                           0.68       207\n",
      "   macro avg       0.58      0.55      0.53       207\n",
      "weighted avg       0.65      0.68      0.62       207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Read a sample of the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Reduce the size of the dataset for quicker experimentation\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df_sample['Text'], df_sample['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV for Multinomial Naive Bayes\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0]}  # Alpha values for Laplace smoothing\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=3)\n",
    "grid_search.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Initialize and train the Multinomial Naive Bayes model with best hyperparameters\n",
    "best_nb_model = MultinomialNB(alpha=best_params['alpha'])\n",
    "best_nb_model.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Predictions on the test set using the tuned model\n",
    "tuned_predictions = best_nb_model.predict(test_tfidf)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "tuned_accuracy = accuracy_score(test_labels, tuned_predictions)\n",
    "tuned_classification_report_result = classification_report(test_labels, tuned_predictions)\n",
    "\n",
    "# Display evaluation metrics after hyperparameter tuning\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"Accuracy after hyperparameter tuning: {tuned_accuracy:.2f}\")\n",
    "print(\"\\nClassification Report after hyperparameter tuning:\")\n",
    "print(tuned_classification_report_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\python310\\lib\\site-packages (3.0.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\python310\\lib\\site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\python310\\lib\\site-packages (from flask) (3.0.1)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\python310\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\python310\\lib\\site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\python310\\lib\\site-packages (from flask) (1.7.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gunicorn in c:\\python310\\lib\\site-packages (21.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\divya\\appdata\\roaming\\python\\python310\\site-packages (from gunicorn) (23.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gunicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (1.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before hyperparameter tuning: 0.72\n",
      "\n",
      "Classification Report before hyperparameter tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.02      0.04       106\n",
      "      Danish       0.94      0.88      0.91        73\n",
      "       Dutch       1.00      0.93      0.96       111\n",
      "     English       0.97      0.97      0.97       291\n",
      "      French       0.99      0.93      0.96       219\n",
      "      German       0.99      0.92      0.96        93\n",
      "       Greek       0.25      0.01      0.03        68\n",
      "       Hindi       0.00      0.00      0.00        10\n",
      "     Italian       1.00      0.92      0.96       145\n",
      "     Kannada       0.00      0.00      0.00        66\n",
      "   Malayalam       0.00      0.00      0.00       121\n",
      "  Portugeese       0.99      0.92      0.96       144\n",
      "     Russian       0.20      0.99      0.33       136\n",
      "     Spanish       0.97      0.93      0.95       160\n",
      "    Sweedish       0.98      0.88      0.93       133\n",
      "       Tamil       0.00      0.00      0.00        87\n",
      "     Turkish       1.00      0.80      0.89       105\n",
      "\n",
      "    accuracy                           0.72      2068\n",
      "   macro avg       0.66      0.60      0.58      2068\n",
      "weighted avg       0.77      0.72      0.70      2068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Read the preprocessed data from 'preprocessed.csv'\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df['Text'], df['Language'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Initialize and train the Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='linear', decision_function_shape='ovr')  # 'ovr' for multiclass\n",
    "svm_model.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Save the trained model and vectorizer\n",
    "model_path = 'multilingual_detection_model.joblib'\n",
    "vectorizer_path = 'tfidf_vectorizer_multilingual.joblib'\n",
    "joblib.dump(svm_model, model_path)\n",
    "joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = svm_model.predict(test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "classification_report_result = classification_report(test_labels, predictions)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Accuracy before hyperparameter tuning: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report before hyperparameter tuning:\")\n",
    "print(classification_report_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 3))),\n",
       "                (&#x27;clf&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 3))),\n",
       "                (&#x27;clf&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 3))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='char', ngram_range=(1, 3))),\n",
       "                ('clf', LogisticRegression())])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Language']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X1,y, random_state=42)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), analyzer='char')\n",
    "model = pipeline.Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "cm = confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.7485493230174082\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is :\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.00      0.00      0.00       134\n",
      "      Danish       0.94      0.90      0.92        97\n",
      "       Dutch       0.98      0.96      0.97       139\n",
      "     English       0.95      0.99      0.97       364\n",
      "      French       0.98      0.97      0.98       269\n",
      "      German       0.99      0.95      0.97       116\n",
      "       Greek       0.67      0.05      0.09        86\n",
      "       Hindi       0.00      0.00      0.00        13\n",
      "     Italian       0.97      0.95      0.96       180\n",
      "     Kannada       0.00      0.00      0.00        78\n",
      "   Malayalam       0.20      0.02      0.03       158\n",
      "  Portugeese       0.96      0.96      0.96       170\n",
      "     Russian       0.23      1.00      0.38       171\n",
      "     Spanish       0.97      0.95      0.96       206\n",
      "    Sweedish       0.92      0.95      0.94       162\n",
      "       Tamil       0.00      0.00      0.00       114\n",
      "     Turkish       0.95      0.95      0.95       128\n",
      "\n",
      "    accuracy                           0.75      2585\n",
      "   macro avg       0.63      0.62      0.59      2585\n",
      "weighted avg       0.73      0.75      0.71      2585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    lang = model.predict([text])\n",
    "    print('The Language is in',lang[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Language is in English\n",
      "The Language is in French\n",
      "The Language is in Russian\n",
      "The Language is in Spanish\n",
      "The Language is in Russian\n",
      "The Language is in Russian\n",
      "The Language is in Russian\n",
      "The Language is in English\n"
     ]
    }
   ],
   "source": [
    "predict(\"LANGUAGE DETECTION MODEL CHECK\")\n",
    "# French\n",
    "predict(\"VÉRIFICATION DU MODÈLE DE DÉTECTION DE LA LANGUE\")\n",
    "# Arabic\n",
    "predict(\"توففحص نموذج الكشف عن اللغة\")\n",
    "# Spanish\n",
    "predict(\"VERIFICACIÓN DEL MODELO DE DETECCIÓN DE IDIOMAS\")\n",
    "# Malayalam\n",
    "predict(\"ലാംഗ്വേജ് ഡിറ്റക്ഷൻ മോഡൽ ചെക്ക്\")\n",
    "# Russian\n",
    "predict(\"ПРОВЕРКА МОДЕЛИ ОПРЕДЕЛЕНИЯ ЯЗЫКА\")\n",
    "# Hindi\n",
    "predict('भाषा का पता लगाने वाले मॉडल की जांच')\n",
    "# Hindi\n",
    "predict(' boyit9h एनालिटिक्स alhgserog 90980879809 bguytfivb ahgseporiga प्रदान करता है')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "detect(\"this is kiranmai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[te:0.9999990354948718]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect_langs\n",
    "detect_langs(\"ఎలా ఉన్నావ్\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text DetectedLanguage\n",
      "0      ['nature', 'in', 'the', 'broadest', 'sense', '...               es\n",
      "1      ['nature', 'can', 'refer', 'to', 'the', 'pheno...               en\n",
      "2      ['the', 'study', 'of', 'nature', 'is', 'a', 'l...               en\n",
      "3      ['although', 'humans', 'are', 'part', 'of', 'n...               en\n",
      "4      ['the', 'word', 'nature', 'is', 'borrowed', 'f...               en\n",
      "...                                                  ...              ...\n",
      "10332                                                 []               en\n",
      "10333                                                 []               en\n",
      "10334                                                 []               en\n",
      "10335                                            ['ess']               en\n",
      "10336                                                 []               en\n",
      "\n",
      "[10337 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import langid\n",
    "\n",
    "# Load the preprocessed data (or replace with your data loading logic)\n",
    "preprocessed_path = 'preprocessed.csv'\n",
    "df = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Assume 'Text' is the column containing the text data\n",
    "texts = df['Text'].tolist()\n",
    "\n",
    "# Detect languages using langid\n",
    "langid_results = [langid.classify(text) for text in texts]\n",
    "\n",
    "# Extract detected languages\n",
    "detected_languages = [lang for lang, _ in langid_results]\n",
    "\n",
    "# Add detected languages to the dataframe\n",
    "df['DetectedLanguage'] = detected_languages\n",
    "\n",
    "# Display the dataframe with detected languages\n",
    "print(df[['Text', 'DetectedLanguage']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: ja\n"
     ]
    }
   ],
   "source": [
    "import langid\n",
    "\n",
    "# Example text to detect language\n",
    "new_text = \"彼女は寝ています\"\n",
    "\n",
    "# Detect the language using langid\n",
    "detected_language, confidence = langid.classify(new_text)\n",
    "\n",
    "# Print the detected language and confidence score\n",
    "print(f\"Detected Language: {detected_language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: zh\n"
     ]
    }
   ],
   "source": [
    "import langid\n",
    "\n",
    "# Example text to detect language\n",
    "new_text = \"嗨你好你好吗\"\n",
    "\n",
    "# Detect the language using langid\n",
    "detected_language, confidence = langid.classify(new_text)\n",
    "\n",
    "# Print the detected language and confidence score\n",
    "print(f\"Detected Language: {detected_language}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
